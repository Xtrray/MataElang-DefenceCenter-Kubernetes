apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: spark
    part: spark-worker
  name: spark-worker
spec:
  serviceName: spark-worker-service
  replicas: 2
  selector:
    matchLabels:
      part: spark-worker
  template:
    metadata:
      labels:
        part: spark-worker
    spec:
      containers:
        - args:
            - bash
            - -c
            - /opt/spark/sbin/start-worker.sh $SPARK_MASTER
          env:
            - name: HADOOP_USER_NAME
              valueFrom:
                configMapKeyRef:
                  key: HADOOP_USER_NAME
                  name: spark-env
            - name: SPARK_APP_JAR_PATH
              valueFrom:
                configMapKeyRef:
                  key: SPARK_APP_JAR_PATH
                  name: spark-env
            - name: SPARK_EVENTLOG_DIR
              valueFrom:
                configMapKeyRef:
                  key: SPARK_EVENTLOG_DIR
                  name: spark-env
            - name: SPARK_HISTORY_OPTS
              valueFrom:
                configMapKeyRef:
                  key: SPARK_HISTORY_OPTS
                  name: spark-env
            - name: SPARK_MASTER
              value: spark://spark-master-service.default.svc.cluster.local:7077
            - name: SPARK_NO_DAEMONIZE
              value: "true"
            - name: SPARK_SCALA_VERSION
              value: "2.13"
            - name: SPARK_WORKER_CORES
              value: "2"
            - name: SPARK_WORKER_DIR
              value: /opt/spark/work-dir
            - name: SPARK_WORKER_MEMORY
              value: 4G
          image: mataelang/spark:3.3.1-scala2.13
          name: spark-worker
          ports:
            - containerPort: 8081
            - containerPort: 4040
            - containerPort: 4041
          resources: {}
          volumeMounts:
            - name: spark-conf
              mountPath: /opt/spark/conf/
              readOnly: true
          securityContext:
            runAsUser: 0
            runAsGroup: 0
      restartPolicy: Always
      volumes:
        - name: spark-conf
          configMap:
            name: spark-conf
